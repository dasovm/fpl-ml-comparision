{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time_series_utils as utils\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 10\n",
    "\n",
    "# Import data instead of generation\n",
    "X, targets, y = utils.generate_data(time_steps, 2, using_classes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_targets, train_y, val_X, val_targets, val_y = utils.split_df_to_train_test(X, targets, y, split_rate=0.6)\n",
    "val_X, val_targets, val_y, test_X, test_targets, test_y = utils.split_df_to_train_test(val_X, val_targets, val_y, split_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_of_features = train_X[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X, scaler):\n",
    "    return scaler.transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_X.reshape(-1, train_X.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_layers_list = [1, 2, 3]\n",
    "n_neurons_list = [64, 128, 256]\n",
    "n_neurons_last_layer_list = [64, 128, 256]\n",
    "\n",
    "best_config = {\n",
    "    'n_hidden_layers': -1,\n",
    "    'n_neurons': -1,\n",
    "    'n_neurons_last_layer': -1,\n",
    "    'loss': 100000,\n",
    "    'model': None,\n",
    "    'model_history': None\n",
    "}\n",
    "\n",
    "sklearn_weights = class_weight.compute_class_weight('balanced', np.unique(train_y), train_y)\n",
    "weights = dict(enumerate(sklearn_weights))\n",
    "\n",
    "Path(\"lstm-clf-models\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for n_hidden_layers in n_hidden_layers_list:\n",
    "    for n_neurons in n_neurons_list:\n",
    "        for n_neurons_last_layer in n_neurons_last_layer_list:\n",
    "            print('Training {} hidden layers with {} neurons ({} neurons in last layer)'.format(n_hidden_layers, n_neurons, n_neurons_last_layer))\n",
    "            es = EarlyStopping(monitor='val_loss', patience=50)\n",
    "            mcp_save = ModelCheckpoint('lstm-clf-models/model-cp.h5', save_best_only=True, save_weights_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "            model = utils.CondLSTMModel(time_steps, train_X.shape[-1], n_neurons=n_neurons, n_hidden_layers=n_hidden_layers, n_neurons_last_layer=n_neurons_last_layer, using_classes=True)\n",
    "            model.call([scale(train_X, scaler), train_targets])\n",
    "            model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            history = model.fit(x=[scale(train_X, scaler), train_targets], y=train_y, validation_data=([scale(val_X, scaler), val_targets]), epochs=1000, verbose=0, callbacks=[es, mcp_save], class_weight=weights)\n",
    "            \n",
    "            model.load_weights('lstm-clf-models/model-cp.h5')\n",
    "            \n",
    "            metrics = model.evaluate([scale(val_X, scaler), val_targets], val_y, verbose=0)\n",
    "            loss = metrics[0]\n",
    "            print('Training done. Val loss: {:.2f}'.format(loss))\n",
    "            \n",
    "            if best_config['loss'] >= loss:\n",
    "                print('Best setup so far! (val loss: {})'.format(loss))\n",
    "                best_config['n_hidden_layers'] = n_hidden_layers\n",
    "                best_config['n_neurons'] = n_neurons\n",
    "                best_config['n_neurons_last_layer'] = n_neurons_last_layer\n",
    "                best_config['loss'] = loss\n",
    "                best_config['model'] = model\n",
    "                best_config['model_history'] = history\n",
    "                model.save_weights('lstm-clf-models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = best_config['model_history']\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = utils.CondLSTMModel(time_steps, train_X.shape[-1], n_neurons=best_config['n_neurons'], n_hidden_layers=best_config['n_hidden_layers'], n_neurons_last_layer=best_config['n_neurons_last_layer'])\n",
    "model.call([scale(train_X, scaler), train_targets])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.load_weights('lstm-clf-models/model.h5')\n",
    "model.evaluate([scale(test_X, scaler), test_targets], test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
