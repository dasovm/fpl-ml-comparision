{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Model (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import non_time_series_utils as utils\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import regularizers\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data instead of generation\n",
    "df = utils.generate_df()\n",
    "df['total_points'] = utils.get_classes_from_y(df['total_points'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = utils.split_df_to_train_test(df, split_rate=0.6)\n",
    "test_df, val_df = utils.split_df_to_train_test(test_df, split_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = utils.split_df_to_X_y(train_df)\n",
    "val_X, val_y = utils.split_df_to_X_y(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, n_hidden_layers, n_neurons):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_neurons, input_shape=input_shape, activation='relu', kernel_regularizer=regularizers.l2(0.001), bias_regularizer=regularizers.l2(0.001)))\n",
    "\n",
    "    for i in range(0, n_hidden_layers):\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(n_neurons, activation='relu', kernel_regularizer=regularizers.l2(0.001), bias_regularizer=regularizers.l2(0.001)))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_layers_list = [1, 2, 3, 4, 5, 6]\n",
    "n_neurons_list = [64, 128, 256, 512]\n",
    "\n",
    "best_config = {\n",
    "    'n_hidden_layers': -1,\n",
    "    'n_neurons': -1,\n",
    "    'loss': 100000,\n",
    "    'model_history': None\n",
    "}\n",
    "\n",
    "sklearn_weights = class_weight.compute_class_weight('balanced', np.unique(train_y), train_y)\n",
    "weights = dict(enumerate(sklearn_weights))\n",
    "\n",
    "Path(\"mlp-clf-models\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for n_hidden_layers in n_hidden_layers_list:\n",
    "    for n_neurons in n_neurons_list:\n",
    "        print('Training {} hidden layers with {} neurons'.format(n_hidden_layers, n_neurons))\n",
    "        \n",
    "        es = EarlyStopping(monitor='val_loss', patience=50)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, verbose=0)\n",
    "        mc = ModelCheckpoint('./mlp-clf-models/model-cp.h5', monitor='val_loss', save_best_only=True)\n",
    "        \n",
    "        model = build_model((train_X.shape[1],), n_hidden_layers, n_neurons)\n",
    "        \n",
    "        history = model.fit(scaler.transform(train_X), train_y, \n",
    "            verbose=0,\n",
    "            validation_data=(scaler.transform(val_X), val_y), \n",
    "            epochs=1000, \n",
    "            callbacks=[es, mc, reduce_lr],\n",
    "            class_weight=weights,\n",
    "        )\n",
    "        \n",
    "        model = load_model('./mlp-clf-models/model-cp.h5')\n",
    "        loss = model.evaluate(val_X, val_y, verbose=0)\n",
    "        print('Training done. Val loss: {:.2f}'.format(loss[0]))\n",
    "\n",
    "        if best_config['loss'] >= loss[0]:\n",
    "            print('Best setup so far!')\n",
    "            best_config['n_hidden_layers'] = n_hidden_layers\n",
    "            best_config['n_neurons'] = n_neurons\n",
    "            best_config['loss'] = loss[0]\n",
    "            best_config['model_history'] = history\n",
    "            model.save('./mlp-clf-models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = best_config['model_history']\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X, test_y = utils.split_df_to_X_y(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./mlp-clf-models/model.h5')\n",
    "model.evaluate(scaler.transform(test_X), test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
